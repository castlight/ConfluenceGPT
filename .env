
# Vector DB for hosting  vector index from the PDF documents present in the folder represented by property SOURCE_DOCUMENTS
PERSIST_DIRECTORY_PDF=PDF_DB

# Vector DB for hosting vector index from confluence pages
PERSIST_DIRECTORY_CONFLUENCE=db

# For Docker based deployments, uncomment below property and comment the next one. The value is name of shared volume path.
#COMMON_PARENT_PATH=/genaidata

# Uncomment below and comment above for local installation
COMMON_PARENT_PATH=./genaidata

#Embedding model details. You can change this value as per your needs.
EMBEDDINGS_MODEL_NAME=sentence-transformers/average_word_embeddings_glove.6B.300d

#Used for testing with a smaller subset of PDF documents e.g. for specific demos
SOURCE_DOCUMENTS=mydocs

#Document loader details for chunking etc. These values are used for creating right size chunks from paragraphs in confluence.
TARGET_SOURCE_CHUNKS=4
EMBEDDING_CACHE_FOLDER=embeddings
NO_OF_DOCS_TO_SEARCH_FROM=3
# Under parent child retriever strategy(refer Langchain Parent retriever page), these sizes represent number of characters in parent and child paragraphs. These sizes affect the accuracy of finding right paragraphs as context and hence etermine accurcy of the answers.
PARENT_PARAGRAPH_SIZE=1200
CHILD_PARAGRAPH_SIZE=350

#LLM params.
MODEL_N_CTX=3500
N_PREDICT=1500


# Confluence details. Please update below fields as per your company
CONFLUENCE_URL=https://<yourCompany>.atlassian.net/wiki
CONFLUENCE_USERNAME=<your_email_id>@<your_company_domain>
# Generate your confluence API keys at 'https://id.atlassian.com/manage-profile/security/api-tokens' OR Get your confluence API key from your Confluence admin. Once you have the API key, execute the command -
# 'python3 ./src/utils/encryptionHelper.py <confluence_API_key>'
# This shall generate the encrypted keys and update the below two properties 'CONFLUENCE_APIKEY_ENCKEY' and 'CONFLUENCE_PASSWORD_OR_APIKEY'
CONFLUENCE_APIKEY_ENCKEY=
CONFLUENCE_PASSWORD_OR_APIKEY=

# Below is the tested default  LLM. Change it with your choice of LLM and test if its results are better for your use case.
LLM_MODEL=dolphin-llama2-7b.ggmlv3.q3_K_M.bin
LLM_MODEL_URL=https://huggingface.co/TheBloke/Dolphin-Llama2-7B-GGML/resolve/main/dolphin-llama2-7b.ggmlv3.q3_K_M.bin


#Fix for the timeout issue(long running requests (those going beyond 50 seconds) return 504 to the UI). Below settings works with websockets only
client.connection.timeout=3000000
client.connection.connect-timeout=3000000


